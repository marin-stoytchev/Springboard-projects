{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinkedIn Job Scraper\n",
    "\n",
    "Below is a code for scraping LinkedIn job postings without being logged in - anonymous search. The reason is that in LinkedIn Terms of Service using any programs for automated extraction of data from the site is prohibited and could result in account suspension.\n",
    "\n",
    "There are two major drawbacks of performing LinkedIn job search without being logged in:\n",
    "- On many occasions, the search page has difficulty displaying all results. Because of this it is advisable to do a limited search at a time.\n",
    "- Unlike in the case when one performs job search while logged in with their LinkedIn account, in anonymous search the exact number of applicants on many occasions is not provided. There is simply a sentence saying that you will be among the first 25 applicants. Since the number of applicants is one of the valuable pieces of information provided in LinkedIn job search, this appears to be a serious drawback. However, we believe that despite this it is worth it to use anonymous scraping because of the large amount of postings one can collect information from.\n",
    "\n",
    "For our particular project, we perform scraping of job postings from searches for full-time Data Scientist positions in several US metropolitan areas. In order to minimize the impact on the limited number of postings displayed in a single search, searches are performed for postings within the last week and separately for each seniority level - entry, associate, and senior.\n",
    "\n",
    "The scraper uses Selenium with Chrome driver. Special credit is due to Omer Sakarya whose tutorial (https://towardsdatascience.com/selenium-tutorial-scraping-glassdoor-com-in-10-minutes-3d0915c6d905) has been very valuable in building my own scraper. \n",
    "\n",
    "Python regex is used to extract relevant information from the job description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries and packages\n",
    "\n",
    "# use Selenium to get various content from the job postings\n",
    "from selenium.common.exceptions import NoSuchElementException, ElementClickInterceptedException\n",
    "from selenium import webdriver\n",
    "\n",
    "# necessary packages for extracting, processing, and saving data \n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# use these to create some random breaks in the process of scraping to mimic human activity\n",
    "import time\n",
    "from random import randrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use chromedriver with Selenium\n",
    "driver = webdriver.Chrome('/Users/marin/chromedriver_win32/chromedriver') # provide path where chromedriver has been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to https://www.linkedin.com/\n",
    "driver.get('https://www.linkedin.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to job search page - perform the search manually first and copy the link below\n",
    "\n",
    "driver.get('https://www.linkedin.com/jobs/search?keywords=Data%20Scientist&location=Greater%2BChicago%2BArea&geoId=90000014&trk=public_jobs_jobs-search-bar_search-submit&f_TP=1%2C2&redirect=false&position=1&pageNum=0&f_JT=F&f_E=4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cancel sign-in prompt\n",
    "\n",
    "try:\n",
    "    driver.find_element_by_class_name(\"cta-modal__dismiss-btn\").click()  # clicking the X to cancell\n",
    "except NoSuchElementException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile a list of all job links on the left of the page\n",
    "# there is a glitch which allows only 40-50 jobs to be shown -->\n",
    "# one has to scroll down manually to get the max number of jobs displayed \n",
    "\n",
    "# initiate list for job links\n",
    "job_links = []\n",
    "\n",
    "# find all job links on page\n",
    "elems = driver.find_elements_by_xpath(\"//*[contains(@href, 'full-click')]\")\n",
    "\n",
    "for elem in elems:\n",
    "    job_links.append(elem.get_attribute(\"href\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many job links were collected\n",
    "\n",
    "print(len(job_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first few links - this is mostly for testing; can remove/comment it out for actual scraping\n",
    "\n",
    "n_jobs = 2\n",
    "\n",
    "for i in range(n_jobs):\n",
    "    print(job_links[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job search parameters --> don't forget to change for new search!\n",
    "\n",
    "# list of cities/metro areas\n",
    "# cities = ['atl', 'aus', 'bos', 'chi', 'dal', 'dc', 'hou', 'la', 'nc', 'ny', 'phi', 'phx', 'por', 'sd', 'sea', 'sf']\n",
    "\n",
    "city = 'atl'\n",
    "\n",
    "seniority = 'entry'\n",
    "#seniority = 'associate'\n",
    "#seniority = 'senior'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define search lists for eduacation and data science terms wich will be used with regex to extract relevant information\n",
    "\n",
    "# education key words to find education requirements for the position\n",
    "search_bs_terms = ['B.S.','BS','Bachelor']\n",
    "search_ms_terms = ['M.S.','MS','Master']\n",
    "search_phd_terms = ['PhD','PHD','Ph.D.','Doctor']\n",
    "\n",
    "# data science key words to find whether position description corresponds to data scientist role\n",
    "search_ds_terms = ['data scien', 'machine learning', 'deep learning','unsupervised learning',\n",
    "                   'artificial intelligence','data model','predictive model','data visualization', \n",
    "                   'classification','regression','clustering']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get jobs info\n",
    "jobs = [] # jobs placeholder\n",
    "\n",
    "n_jobs = len(job_links)\n",
    "\n",
    "# initialize education indicators\n",
    "edu_bs = 0\n",
    "edu_ms = 0\n",
    "edu_phd = 0\n",
    "\n",
    "count_ds_terms = 0\n",
    "\n",
    "def get_jobs_info(jobs, job_links, n_jobs, edu_bs, edu_ms, edu_phd, \n",
    "             search_bs_terms, search_ms_terms, search_phd_terms, search_ds_terms, count_ds_terms):\n",
    "    \n",
    "    for i in range(n_jobs):\n",
    "        \n",
    "        # set education indicators to 0 before scanning each position\n",
    "        edu_bs = 0\n",
    "        edu_ms = 0\n",
    "        edu_phd = 0\n",
    "        \n",
    "        # set data science terms counter to 0 before scanning each position\n",
    "        count_ds_terms = 0\n",
    "        \n",
    "        if i%4 == 0:\n",
    "            time.sleep(randrange(6) + 5) # every fourth posting wait 5-10 sec \n",
    "        \n",
    "        print('Job #: ', i+1) # this line is mostly for testing -- can comment out during actual scraping\n",
    "        \n",
    "        driver.get(job_links[i])\n",
    "        \n",
    "        time.sleep(randrange(3) + 3) # random wait of 3 to 5 sec\n",
    "        \n",
    "        # position title\n",
    "        try: \n",
    "            pos_title = driver.find_element_by_xpath('//*[@class=\"topcard__title\"]').text\n",
    "        except NoSuchElementException:\n",
    "            pos_title = 'None' # it is important to set a \"not found value\"\n",
    "        \n",
    "        # company\n",
    "        try:\n",
    "            company = driver.find_element_by_xpath('//*[@class=\"topcard__org-name-link topcard__flavor--black-link\"]').text\n",
    "        except NoSuchElementException:\n",
    "            company = 'None' # it is important to set a \"not found value\"\n",
    "        \n",
    "        # location\n",
    "        try:\n",
    "            location = driver.find_element_by_xpath('//*[@class=\"topcard__flavor topcard__flavor--bullet\"]').text\n",
    "        except NoSuchElementException:\n",
    "            location = 'None' # it is important to set a \"not found value\"\n",
    "        \n",
    "        # time posted\n",
    "        try:\n",
    "            time_posted = driver.find_element_by_xpath('//*[contains(@class, \"posted-time-ago\")]').text\n",
    "        except NoSuchElementException:\n",
    "            time_posted = 'None' # it is important to set a \"not found value\"\n",
    "        \n",
    "        # number of applicants\n",
    "        try:\n",
    "            num_applicants = driver.find_element_by_xpath('//*[contains(@class, \"num-applicants__caption\")]').text\n",
    "        except NoSuchElementException:\n",
    "            num_applicants = 'None' # it is important to set a \"not found value\"\n",
    "        \n",
    "        # get all info below description\n",
    "        try:\n",
    "            industry_info = driver.find_element_by_xpath('//*[@class=\"job-criteria__list\"]').text\n",
    "            industry_info = industry_info.split('\\n')\n",
    "        except NoSuchElementException:\n",
    "            industry_info = 'None' # it is important to set a \"not found value\"\n",
    "        \n",
    "        # expand job description by clicking \"Show more\" button\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//*[@class=\"show-more-less-html__button show-more-less-html__button--more\"]').click()\n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        # get the text of the job description\n",
    "        try:\n",
    "            pos_description = driver.find_elements_by_xpath('//*[@class=\"show-more-less-html__markup\"]')\n",
    "            pos_description = pos_description[0].text\n",
    "        except NoSuchElementException:\n",
    "            pos_description = 'None' # it is important to set a \"not found value\"\n",
    "                \n",
    "        # get education requirements from position description\n",
    "        \n",
    "        # check for Bachelor degree\n",
    "        for term in search_bs_terms:\n",
    "            if re.search(term, pos_description):\n",
    "                edu_bs = 1\n",
    "                \n",
    "        # check for Master degree\n",
    "        for term in search_ms_terms:\n",
    "            if re.search(term, pos_description):\n",
    "                edu_ms = 1\n",
    "        \n",
    "        # check for Doctoral degree\n",
    "        for term in search_phd_terms:\n",
    "            if re.search(term, pos_description):\n",
    "                edu_phd = 1\n",
    "        \n",
    "        # get experience requirements from position description\n",
    "        res_experience = re.findall(r'(\\d+\\s+\\byears|[\\d-]+\\d+\\s+\\byears|[\\d+]+\\s+\\byears)', pos_description)\n",
    "        \n",
    "        # get data science terms count in position description to gauge how relevant the position is\n",
    "        for term in search_ds_terms:\n",
    "            for match in re.finditer(term, pos_description, flags = re.IGNORECASE):\n",
    "                count_ds_terms += 1\n",
    "        \n",
    "        # add job info to jobs\n",
    "        jobs.append({\"Job Title\" : pos_title,\n",
    "                     \"Company Name\" : company,\n",
    "                     \"Location\" : location,\n",
    "                     \"Metro Area\" : city.upper(), \n",
    "                     \"Time Posted\" : time_posted,\n",
    "                     \"Number of Applicants\" : num_applicants,\n",
    "                     \"Industry Info\" : industry_info,\n",
    "                     \"Education-Bachelor\" : edu_bs,\n",
    "                     \"Education-Master\" : edu_ms,\n",
    "                     \"Education-Doctor\" : edu_phd,\n",
    "                     \"Experience\" : res_experience,\n",
    "                     \"Data Science Terms Count\" : count_ds_terms})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform scraping of job postings\n",
    "jobs = []\n",
    "n_jobs = len(job_links)\n",
    "\n",
    "get_jobs_info(jobs, job_links, n_jobs, edu_bs, edu_ms, edu_phd, \n",
    "             search_bs_terms, search_ms_terms, search_phd_terms, search_ds_terms, count_ds_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the collected info to a data frame\n",
    "\n",
    "data = pd.DataFrame(jobs)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save job links to excel file to allow for possibility to use later\n",
    "\n",
    "links_filename = 'raw_data_5/joblinks_ds_' + city + '_' + seniority + '_5.xlsx'\n",
    "\n",
    "data_links = pd.DataFrame(job_links)\n",
    "data_links.to_excel(links_filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save job data as an excel file\n",
    "jobs_filename = 'raw_data_5/jobs_ds_' + city + '_' + seniority + '_5.xlsx'\n",
    "\n",
    "data.to_excel(jobs_filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read saved file to check if it reads correctly\n",
    "\n",
    "data_test = pd.read_excel(jobs_filename)\n",
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_links[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del job_links[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
